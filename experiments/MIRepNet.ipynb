{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DT95WN9DU8u",
        "outputId": "edf65e74-49e6-4b66-bb42-b9fc69366ef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m0.0/7.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏―u001b[0m\u001b[90m笊ｺ\u001b[0m\u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m1.0/7.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m\u001b[91m笊ｸ\u001b[0m \u001b[32m7.4/7.5 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install mne --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "py-RKL4sFOgB",
        "outputId": "5cff713d-8463-4542-b08b-3c2d1659a00f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.9.0+cu126\n",
            "Uninstalling torch-2.9.0+cu126:\n",
            "  Successfully uninstalled torch-2.9.0+cu126\n",
            "Found existing installation: torchvision 0.24.0+cu126\n",
            "Uninstalling torchvision-0.24.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.24.0+cu126\n",
            "Found existing installation: torchaudio 2.9.0+cu126\n",
            "Uninstalling torchaudio-2.9.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.9.0+cu126\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp312-cp312-linux_x86_64.whl (811.6 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m811.6/811.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.17.0%2Bcu118-cp312-cp312-linux_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.2.0%2Bcu118-cp312-cp312-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m126.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.7.0.84 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m813.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.19.3 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.19.3-py3-none-manylinux1_x86_64.whl (135.3 MB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m135.3/135.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (2.32.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.17.0) (11.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchvision==0.17.0) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 torch-2.2.0+cu118 torchaudio-2.2.0+cu118 torchvision-0.17.0+cu118\n"
          ]
        }
      ],
      "source": [
        "# Uninstall existing PyTorch\n",
        "!pip uninstall torch torchvision torchaudio -y\n",
        "\n",
        "# Install the stable version for CUDA 11.8 (Adjust if using a different CUDA version)\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku5bDP5r9y5U",
        "outputId": "cd36395b-c48b-4d55-e59b-3fdcd206f1fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'MIRepNet'...\n",
            "remote: Enumerating objects: 331, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 331 (delta 36), reused 15 (delta 15), pack-reused 279 (from 2)\u001b[K\n",
            "Receiving objects: 100% (331/331), 25.03 MiB | 22.07 MiB/s, done.\n",
            "Resolving deltas: 100% (157/157), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/staraink/MIRepNet.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "sReGA4kCJGca",
        "outputId": "a2747183-cb9c-4ec8-f3b9-8f3bf850a05c"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdlFQ9Zs9Uq2",
        "outputId": "fa300b67-5d7c-4c0b-8308-4d03303974f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting memory-optimized loading and epoching, TARGET SFREQ: 128.0Hz...\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S010 processed. Total epochs collected so far: 1740\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S020 processed. Total epochs collected so far: 3480\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S030 processed. Total epochs collected so far: 5220\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S040 processed. Total epochs collected so far: 6948\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S050 processed. Total epochs collected so far: 8684\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S060 processed. Total epochs collected so far: 10421\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S070 processed. Total epochs collected so far: 12156\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S080 processed. Total epochs collected so far: 13872\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S090 processed. Total epochs collected so far: 15660\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "笨 S100 processed. Total epochs collected so far: 17412\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "Used Annotations descriptions: ['T0', 'T1', 'T2']\n",
            "\n",
            "--- FINAL STEP: Stacking all subjects' data ---\n",
            "\n",
            "沁 Total Final Epochs Collected: **18968**\n",
            "\n",
            "沁 Total Final Epochs Collected: **18968**\n",
            "Epochs per class (Final Dataset): Counter({0: 9184, 1: 4896, 2: 3254, 3: 1634})\n",
            "\n",
            "| Class ID | Class Name | Epoch Count |\n",
            "| :--- | :--- | :--- |\n",
            "| 0 | Rest | 9184 |\n",
            "| 1 | Left | 4896 |\n",
            "| 2 | Right | 3254 |\n",
            "| 3 | Forward | 1634 |\n",
            "\n",
            "Final data shape: **(18968, 64, 539)** (Epochs, Channels, Times)\n",
            "Expected number of time points at 128Hz: 539\n",
            "\n",
            "--- Data Split Shapes ---\n",
            "Train shapes: X=(11380, 64, 539), y=(11380,)\n",
            "Validation shapes: X=(3794, 64, 539), y=(3794,)\n",
            "Test shapes: X=(3794, 64, 539), y=(3794,)\n",
            "\n",
            "笨 Weighted Random Sampler created for balanced training.\n",
            "\n",
            "PyTorch DataLoaders created.\n",
            "The script is maximally memory efficient for an in-memory solution. Ready for model training.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import mne\n",
        "import os\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import warnings\n",
        "import gc\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# =========================================================================\n",
        "# I. CONFIGURATION\n",
        "# =========================================================================\n",
        "\n",
        "TOTAL_SUBJECTS = 109\n",
        "TARGET_SFREQ = 128.0\n",
        "base_data_path = Path('/content/drive/MyDrive/EEG_Processed_Data')\n",
        "\n",
        "# Runs Configuration\n",
        "runs_in_order = [7, 8, 9, 10, 13, 14]\n",
        "hand_runs = [7, 8, 13, 14]\n",
        "feet_runs = [9, 10]\n",
        "expected_num_runs = len(runs_in_order)\n",
        "event_id_4class = { 'Rest':0, 'Left':1, 'Right':2, 'Forward':3 }\n",
        "min_segment_len = 5\n",
        "\n",
        "# =========================================================================\n",
        "# II. MAIN PROCESSING LOOP: Load, Epoch, and Collect NumPy Arrays (Memory Optimized)\n",
        "# =========================================================================\n",
        "\n",
        "all_X = []\n",
        "all_y = []\n",
        "total_epochs_count = 0\n",
        "\n",
        "print(f\"Starting memory-optimized loading and epoching, TARGET SFREQ: {TARGET_SFREQ}Hz...\")\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    for i in range(1, TOTAL_SUBJECTS + 1):\n",
        "        subject_name = f'S{i:03d}'\n",
        "        subject_folder = base_data_path / subject_name\n",
        "        cleaned_file = subject_folder / f\"{subject_name}_cleaned_raw.fif\"\n",
        "\n",
        "        if not cleaned_file.exists():\n",
        "            if i % 10 == 1:\n",
        "                print(f\"[SKIP] {subject_name}: cleaned file not found at {cleaned_file}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            combined_raw = mne.io.read_raw_fif(cleaned_file, preload=True, verbose=False)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] {subject_name}: failed to read {cleaned_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # --- Resampling ---\n",
        "        if combined_raw.info['sfreq'] != TARGET_SFREQ:\n",
        "            combined_raw.resample(TARGET_SFREQ, npad='auto', verbose=False)\n",
        "        sfreq = combined_raw.info['sfreq']\n",
        "\n",
        "        # --- Epoching Logic (omitted for brevity, remains the same) ---\n",
        "        annotations = combined_raw.annotations\n",
        "        if annotations is None or len(annotations) == 0:\n",
        "            del combined_raw; gc.collect(); continue\n",
        "        try:\n",
        "            events, event_dict = mne.events_from_annotations(combined_raw)\n",
        "        except Exception as e:\n",
        "            del combined_raw; gc.collect(); continue\n",
        "\n",
        "        # Run Segmentation (merge boundary sections)\n",
        "        boundary_labels = {'BAD boundary', 'EDGE boundary'}\n",
        "        desc_list = [str(a['description']) for a in annotations]\n",
        "        boundary_idx = [idx for idx, d in enumerate(desc_list) if d in boundary_labels]\n",
        "\n",
        "        segment_edges = [-1] + boundary_idx + [len(desc_list) - 1]\n",
        "        segments = []\n",
        "        for s in range(len(segment_edges) - 1):\n",
        "            start = segment_edges[s] + 1\n",
        "            end = segment_edges[s + 1]\n",
        "            if start <= end:\n",
        "                segments.append((start, end))\n",
        "\n",
        "        merged = []\n",
        "        for s, e in segments:\n",
        "            length = e - s + 1\n",
        "            if len(merged) > 0 and length < min_segment_len:\n",
        "                last_s, last_e = merged.pop()\n",
        "                merged.append((last_s, e))\n",
        "            else:\n",
        "                merged.append((s, e))\n",
        "        segments = merged\n",
        "\n",
        "        if len(segments) != expected_num_runs:\n",
        "            if i % 10 == 0:\n",
        "                print(f\"[SEGMENT FAIL] {subject_name}: found {len(segments)} segments (expected {expected_num_runs}) - skipping\")\n",
        "            del combined_raw; gc.collect(); continue\n",
        "\n",
        "        full_run_list = np.empty(len(desc_list), dtype=int)\n",
        "        for seg_idx, (s, e) in enumerate(segments):\n",
        "            runnum = runs_in_order[seg_idx]\n",
        "            full_run_list[s:e+1] = runnum\n",
        "\n",
        "        # Context-Dependent Event Mapping (T0/T1/T2 -> 0/1/2/3)\n",
        "        mapped_events = events.copy()\n",
        "\n",
        "        for ann_idx, ann in enumerate(annotations):\n",
        "            desc = str(ann['description'])\n",
        "            try:\n",
        "                run = int(full_run_list[ann_idx])\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            if desc not in ('T0', 'T1', 'T2'):\n",
        "                continue\n",
        "\n",
        "            onset_sec = ann['onset']\n",
        "            sample = int(round(onset_sec * sfreq))\n",
        "\n",
        "            if desc == 'T0':\n",
        "                new_val = 0\n",
        "            elif desc == 'T1':\n",
        "                new_val = 1\n",
        "            elif desc == 'T2':\n",
        "                new_val = 2 if run in hand_runs else 3\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            close_idx = np.where(np.abs(events[:, 0] - sample) <= 1)[0]\n",
        "            if close_idx.size > 0:\n",
        "                mapped_events[close_idx, 2] = new_val\n",
        "            else:\n",
        "                nearest_idx = np.argmin(np.abs(events[:, 0] - sample))\n",
        "                mapped_events[nearest_idx, 2] = new_val\n",
        "\n",
        "        # 4. Create Epochs, Drop Bad, and COLLECT NUMPY ARRAYS\n",
        "        try:\n",
        "            epochs_subject = mne.Epochs(\n",
        "                combined_raw, mapped_events, event_id=event_id_4class,\n",
        "                tmin=-0.2, tmax=4.0, baseline=(-0.2, 0), preload=True, verbose=False\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] {subject_name}: creating Epochs failed: {e}\")\n",
        "            del combined_raw; gc.collect(); continue\n",
        "\n",
        "        try:\n",
        "            epochs_subject.drop_bad()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        num_epochs = len(epochs_subject)\n",
        "        if num_epochs > 0:\n",
        "            # 泅ｨ CRITICAL OPTIMIZATION: Convert to float32 here to halve memory!\n",
        "            X_data = epochs_subject.get_data().astype(np.float32)\n",
        "            all_X.append(X_data)\n",
        "            all_y.append(epochs_subject.events[:, 2])\n",
        "            total_epochs_count += num_epochs\n",
        "\n",
        "        # clean up large objects & force GC\n",
        "        del combined_raw\n",
        "        del epochs_subject\n",
        "        gc.collect()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"笨 {subject_name} processed. Total epochs collected so far: {total_epochs_count}\")\n",
        "\n",
        "# =========================================================================\n",
        "# III. FINAL CONCATENATION AND VERIFICATION (Single Stack)\n",
        "# =========================================================================\n",
        "\n",
        "print(\"\\n--- FINAL STEP: Stacking all subjects' data ---\")\n",
        "\n",
        "if total_epochs_count == 0:\n",
        "    raise RuntimeError(\"No epochs were collected. Check file paths and loading.\")\n",
        "\n",
        "X_final = np.concatenate(all_X, axis=0)\n",
        "y_final = np.concatenate(all_y, axis=0)\n",
        "\n",
        "# Free up lists\n",
        "del all_X, all_y\n",
        "gc.collect()\n",
        "\n",
        "# Verification prints (unchanged)\n",
        "expected_times = int(np.round((4.0 - (-0.2)) * TARGET_SFREQ)) + 1\n",
        "print(f\"\\n沁 Total Final Epochs Collected: **{X_final.shape[0]}**\")\n",
        "print(f\"\\n沁 Total Final Epochs Collected: **{X_final.shape[0]}**\")\n",
        "\n",
        "class_counts = Counter(y_final)\n",
        "print(\"Epochs per class (Final Dataset):\", class_counts)\n",
        "\n",
        "print(\"\\n| Class ID | Class Name | Epoch Count |\")\n",
        "print(\"| :--- | :--- | :--- |\")\n",
        "print(f\"| 0 | Rest | {class_counts.get(0, 0)} |\")\n",
        "print(f\"| 1 | Left | {class_counts.get(1, 0)} |\")\n",
        "print(f\"| 2 | Right | {class_counts.get(2, 0)} |\")\n",
        "print(f\"| 3 | Forward | {class_counts.get(3, 0)} |\")\n",
        "print(f\"\\nFinal data shape: **{X_final.shape}** (Epochs, Channels, Times)\")\n",
        "print(f\"Expected number of time points at 128Hz: {expected_times}\")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# IV. SPLITTING AND STANDARDIZATION (MAX MEMORY OPTIMIZED)\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "## 洫 Splitting and Standardization (MAX Memory Optimized)\n",
        "\n",
        "# --- 1. Data Splitting (Train/Validation/Test) ---\n",
        "# Splitting inherently creates three copies, which is unavoidable.\n",
        "X_train_temp, X_test, y_train_temp, y_test = train_test_split(\n",
        "    X_final, y_final, test_size=0.2, random_state=42, stratify=y_final\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_temp, y_train_temp, test_size=0.25, random_state=42, stratify=y_train_temp\n",
        ")\n",
        "\n",
        "# Delete large intermediate copies immediately\n",
        "del X_final, y_final, X_train_temp, y_train_temp\n",
        "gc.collect()\n",
        "\n",
        "print(f\"\\n--- Data Split Shapes ---\")\n",
        "print(f\"Train shapes: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Validation shapes: X={X_val.shape}, y={y_val.shape}\")\n",
        "print(f\"Test shapes: X={X_test.shape}, y={y_test.shape}\")\n",
        "\n",
        "\n",
        "# --- 2. Scaling (Standardization per Channel-Time Point) ---\n",
        "scaler = StandardScaler()\n",
        "\n",
        "def scale_and_cleanup(X_data, scaler, fit=False):\n",
        "    \"\"\"\n",
        "    Applies scaling and converts the array to a PyTorch tensor.\n",
        "    Returns a tensor shaped (N, T, C) where:\n",
        "      N = epochs, T = timesteps, C = channels/features.\n",
        "    \"\"\"\n",
        "    # X_data: (N, C, T) float32\n",
        "    N, C, T = X_data.shape\n",
        "\n",
        "    # 1. Reshape for scaling: (N, C, T) -> transpose to (N, T, C) -> reshape to (N*T, C)\n",
        "    X_for_scaler = np.ascontiguousarray(X_data).transpose(0, 2, 1).reshape(-1, C)  # shape (N*T, C)\n",
        "\n",
        "    # 2. Fit/Transform\n",
        "    if fit:\n",
        "        X_scaled_flat = scaler.fit_transform(X_for_scaler)\n",
        "    else:\n",
        "        X_scaled_flat = scaler.transform(X_for_scaler)\n",
        "\n",
        "    # 3. Reshape back to (N, T, C)\n",
        "    X_scaled = X_scaled_flat.reshape(N, T, C)\n",
        "\n",
        "    # 4. Convert to PyTorch Tensor with dtype float32\n",
        "    X_tensor = torch.as_tensor(X_scaled, dtype=torch.float32)\n",
        "\n",
        "    # 5. Cleanup\n",
        "    del X_for_scaler, X_scaled_flat, X_scaled\n",
        "    gc.collect()\n",
        "\n",
        "    return X_tensor  # shape (N, T, C)\n",
        "\n",
        "\n",
        "# Process Training Data\n",
        "X_train_tensor = scale_and_cleanup(X_train, scaler, fit=True)\n",
        "del X_train\n",
        "gc.collect()\n",
        "\n",
        "# Process Validation Data\n",
        "X_val_tensor = scale_and_cleanup(X_val, scaler, fit=False)\n",
        "del X_val\n",
        "gc.collect()\n",
        "\n",
        "# Process Test Data\n",
        "X_test_tensor = scale_and_cleanup(X_test, scaler, fit=False)\n",
        "del X_test\n",
        "gc.collect()\n",
        "\n",
        "# Convert y arrays to tensors after X data is processed\n",
        "y_train_tensor = torch.as_tensor(y_train, dtype=torch.long)\n",
        "y_val_tensor = torch.as_tensor(y_val, dtype=torch.long)\n",
        "y_test_tensor = torch.as_tensor(y_test, dtype=torch.long)\n",
        "\n",
        "del y_train, y_val, y_test\n",
        "gc.collect()\n",
        "\n",
        "# =========================================================================\n",
        "# V. DATA BALANCING (Weighted Random Sampler)\n",
        "# =========================================================================\n",
        "\n",
        "# The counts are taken directly from your output:\n",
        "class_counts = Counter({0: 9184, 1: 4896, 2: 3254, 3: 1634})\n",
        "total_epochs = sum(class_counts.values()) # 18968\n",
        "\n",
        "# --- 1. Calculate Inverse Frequency Weights ---\n",
        "# The weight for a class is inversely proportional to its frequency.\n",
        "# weight_i = 1.0 / count_i\n",
        "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
        "\n",
        "# --- 2. Map Weights to Every Sample in the Training Set ---\n",
        "# Create a list of weights corresponding to the label of every epoch in y_train_tensor\n",
        "sample_weights = y_train_tensor.clone().detach().cpu().numpy() # Convert labels to numpy array\n",
        "\n",
        "# Replace the label index (0, 1, 2, 3) with its corresponding calculated weight\n",
        "for cls, weight in class_weights.items():\n",
        "    sample_weights[sample_weights == cls] = weight\n",
        "\n",
        "# Convert weights back to a PyTorch tensor\n",
        "sample_weights = torch.from_numpy(sample_weights).double()\n",
        "\n",
        "# --- 3. Create Weighted Random Sampler ---\n",
        "# The number of samples to draw in one pass (num_samples) is set to the size\n",
        "# of the training dataset to ensure a full epoch run.\n",
        "sampler = torch.utils.data.WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights), # The number of elements to draw (equals dataset size)\n",
        "    replacement=True # Must be True for oversampling the minority classes\n",
        ")\n",
        "\n",
        "print(\"\\n笨 Weighted Random Sampler created for balanced training.\")\n",
        "\n",
        "\n",
        "# --- 3. Convert to PyTorch Tensors and DataLoaders ---\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "NUM_WORKERS = 4  # adjust to your environment; 0 on Windows without fork\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sampler=sampler,       # use sampler for balanced sampling\n",
        "    shuffle=False,         # must be False when sampler is provided\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "print(\"\\nPyTorch DataLoaders created.\")\n",
        "print(\"The script is maximally memory efficient for an in-memory solution. Ready for model training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2ht4APE0A13V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "import wandb\n",
        "import math\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, embed_dim=128, num_channels=64, input_timesteps=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_channels = num_channels\n",
        "        self.embed_dim = embed_dim\n",
        "        self.input_timesteps = input_timesteps\n",
        "\n",
        "        # Adaptive convolution parameters based on input size\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=(1, 25), stride=(1, 1), padding=(0, 12))\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(self.num_channels, 1), stride=(1, 1))\n",
        "        self.bn = nn.BatchNorm2d(128)\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "        # Adaptive pooling - we'll calculate the kernel size dynamically\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))  # Keep temporal dimension flexible\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(128, embed_dim, (1, 1), stride=(1, 1)),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x shape: (B, T, C) -> we need (B, 1, C, T)\n",
        "        x = x.unsqueeze(1)  # (B, 1, C, T)\n",
        "        B, _, C, T = x.size()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.elu(x)\n",
        "\n",
        "        # Use adaptive pooling instead of fixed pooling\n",
        "        x = self.adaptive_pool(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.projection(x)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Tensor = None) -> Tensor:\n",
        "        queries = rearrange(self.queries(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        keys = rearrange(self.keys(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        values = rearrange(self.values(x), \"b n (h d) -> b h n d\", h=self.num_heads)\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "        scaling = self.emb_size ** (1 / 2)\n",
        "        att = F.softmax(energy / scaling, dim=-1)\n",
        "        att = self.att_drop(att)\n",
        "        out = torch.einsum('bhal, bhlv -> bhav', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x\n",
        "\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion, drop_p):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, num_heads=8, drop_p=0.5, forward_expansion=4, forward_drop_p=0.5):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, num_heads, drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            ))\n",
        "        )\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth, emb_size,dropout=0.5):\n",
        "        super().__init__(*[TransformerEncoderBlock(emb_size,drop_p=dropout) for _ in range(depth)])\n",
        "\n",
        "\n",
        "class decoder(nn.Module):\n",
        "    def __init__(self, emb_size=64, depth=2, pretrain=None,**kwargs):\n",
        "        super().__init__()\n",
        "        self.transformer = TransformerEncoder(depth, emb_size)\n",
        "    def forward(self, x):\n",
        "        x = self.transformer(x)      # [batch_size, seq_length, emb_size]\n",
        "        return x\n",
        "\n",
        "class decoder_fft(nn.Module):\n",
        "    def __init__(self, emb_size=64, depth=2, pretrain=None,**kwargs):\n",
        "        super().__init__()\n",
        "        self.transformer = TransformerEncoder(depth, emb_size)\n",
        "        self.pro= nn.Linear(emb_size, 3*2)\n",
        "    def forward(self, x):\n",
        "        out = self.transformer(x)      # [batch_size, seq_length, emb_size]\n",
        "        out = self.pro(torch.mean(out, dim=1))  # [batch_size, seq_length, 3*2]\n",
        "        return out\n",
        "\n",
        "class mlm_mask(nn.Module):\n",
        "    def __init__(self, emb_size=128, depth=6, n_classes=2,mask_ratio=0.5, pretrain=None,pretrainmode=False):\n",
        "        super().__init__()\n",
        "        self.pretrainmode = pretrainmode\n",
        "        self.embedding = PatchEmbedding(embed_dim=emb_size)\n",
        "        self.transformer = TransformerEncoder(depth, emb_size,dropout=0.5)\n",
        "        self.clshead = nn.Linear(emb_size,n_classes)\n",
        "        self.mask_ratio = mask_ratio\n",
        "        if pretrain is not None:\n",
        "            self.init_from_pretrained(pretrain)\n",
        "\n",
        "        if self.pretrainmode:\n",
        "            self.mask_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "            self.decoder = decoder(emb_size=emb_size, depth=2)\n",
        "\n",
        "    def random_masking(self, x, mask_ratio=0.5):\n",
        "        B, L, D = x.shape\n",
        "        len_keep = int(L * (1 - mask_ratio))\n",
        "\n",
        "        noise = torch.rand(B, L, device=x.device)\n",
        "        ids_shuffle = torch.argsort(noise, dim=1)\n",
        "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
        "\n",
        "        ids_keep = ids_shuffle[:, :len_keep]\n",
        "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).expand(-1, -1, D))\n",
        "\n",
        "        mask_tokens = self.mask_token.repeat(B, L - len_keep, 1)\n",
        "        x_masked = torch.cat([x_masked, mask_tokens], dim=1)\n",
        "\n",
        "        x_masked = torch.gather(x_masked, dim=1, index=ids_restore.unsqueeze(-1).expand(-1, -1, D))\n",
        "\n",
        "        mask = torch.zeros([B, L], device=x.device)\n",
        "        mask[:, :len_keep] = 1\n",
        "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
        "\n",
        "        return x_masked, mask, ids_restore\n",
        "\n",
        "    def forward(self, x):\n",
        "        original_x = self.embedding(x)\n",
        "\n",
        "        if self.pretrainmode:\n",
        "\n",
        "            x_masked, mask, ids_restore = self.random_masking(original_x,mask_ratio=self.mask_ratio)\n",
        "\n",
        "            encoded = self.transformer(x_masked)\n",
        "\n",
        "            reconstructed = self.decoder(encoded)\n",
        "\n",
        "            cls_output = self.clshead(torch.mean(encoded, dim=1))\n",
        "\n",
        "            return cls_output, original_x, reconstructed, None\n",
        "        else:\n",
        "            transformed = self.transformer(original_x)\n",
        "            pooled = torch.mean(transformed, dim=1)\n",
        "            cls_output = self.clshead(pooled)\n",
        "            return pooled, cls_output\n",
        "    def init_from_pretrained(self, pretrained_path, freeze_encoder=False, strict=True):\n",
        "        pretrained_dict = torch.load(pretrained_path)\n",
        "\n",
        "        model_dict = self.state_dict()\n",
        "\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
        "                          if k in model_dict and v.shape == model_dict[k].shape}\n",
        "\n",
        "        model_dict.update(pretrained_dict)\n",
        "\n",
        "        self.load_state_dict(model_dict, strict=strict)\n",
        "\n",
        "        if freeze_encoder:\n",
        "            for name, param in self.named_parameters():\n",
        "                if 'embedding' in name or 'transformer' in name:\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        print(f\"Loaded {len(pretrained_dict)}/{len(model_dict)} parameters from pretrained model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZriE7DKcBKHi"
      },
      "outputs": [],
      "source": [
        "# --- Essential Imports ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from huggingface_hub import hf_hub_download\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# --- Define Constants ---\n",
        "N_CHANNELS = 64\n",
        "N_CLASSES = 4\n",
        "EMB_SIZE = 128\n",
        "DEPTH = 6\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, device, scheduler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for data, labels in train_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "        _, outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = correct / total * 100\n",
        "\n",
        "    return epoch_loss, accuracy, current_lr\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in val_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            _, outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    accuracy = correct / total * 100\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BewWOAwFBKoE",
        "outputId": "458f363e-b90e-48fc-8c24-4a0838e2c929"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pre-trained weights downloaded to: /root/.cache/huggingface/hub/models--starself--MIRepNet/snapshots/9bac0439c0d3e9ffdb40ca675d61a51b439a446e/MIRepNet.pth\n",
            "Loaded 8/110 parameters from pretrained model\n",
            "Model initialized and pre-trained backbone weights loaded.\n"
          ]
        }
      ],
      "source": [
        "# 3.1 Download Pre-trained Weights\n",
        "weight_path = hf_hub_download(repo_id=\"starself/MIRepNet\", filename=\"MIRepNet.pth\")\n",
        "print(f\"Pre-trained weights downloaded to: {weight_path}\")\n",
        "\n",
        "# 3.2 Instantiate the MIRepNet (mlm_mask)\n",
        "# Ensure N_CLASSES=4 is passed for your task!\n",
        "model = mlm_mask(\n",
        "    emb_size=EMB_SIZE,\n",
        "    depth=DEPTH,\n",
        "    n_classes=N_CLASSES,\n",
        "    pretrainmode=False,\n",
        "    pretrain=weight_path\n",
        ").to(device)\n",
        "\n",
        "print(\"Model initialized and pre-trained backbone weights loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN2YPGRnOcpa",
        "outputId": "95c4f06b-5118-462d-d5ac-70ec515c961a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "笨 Backbone (Embedding & Transformer) frozen.\n",
            "笨 Classification Head (clshead) is unfrozen and ready for training.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. FREEZE THE ENCODER (BACKBONE) ---\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# --- UNFREEZE ONLY THE CLASSIFICATION HEAD ---\n",
        "# The classification head is named 'clshead' in your mlm_mask class definition\n",
        "for name, param in model.named_parameters():\n",
        "    if 'clshead' in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "print(\"笨 Backbone (Embedding & Transformer) frozen.\")\n",
        "print(\"笨 Classification Head (clshead) is unfrozen and ready for training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q6mZ3XuStR8U"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 1e-3  # Smaller LR for fine-tuning\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "# Since you're only training the classification head, use a slightly higher LR\n",
        "optimizer = optim.Adam(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX-c6Xinto-V",
        "outputId": "2fb52c76-08ba-48c0-a3a2-387389c21a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set class distribution: Counter({0: 5510, 1: 2938, 2: 1952, 3: 980})\n",
            "Class weights: {0: 0.5163339382940109, 1: 0.9683458134785569, 2: 1.4574795081967213, 3: 2.9030612244897958}\n",
            "Sample weights - min: 0.5163, max: 2.9031, sum: 11380.0000\n",
            "Any negative weights: False\n",
            "Any zero weights: False\n",
            "After normalization - min: 0.0000, max: 0.0003, sum: 1.0000\n",
            "笨 Weighted Random Sampler created successfully.\n",
            "笨 DataLoaders created successfully.\n",
            "Train loader: 177 batches\n",
            "Val loader: 60 batches\n",
            "Test loader: 60 batches\n"
          ]
        }
      ],
      "source": [
        "# =========================================================================\n",
        "# V. DATA BALANCING (Weighted Random Sampler - FIXED)\n",
        "# =========================================================================\n",
        "\n",
        "# --- 1. Calculate Class Weights Correctly ---\n",
        "# Get class counts from the training set only\n",
        "class_counts = Counter(y_train_tensor.numpy())\n",
        "print(\"Training set class distribution:\", class_counts)\n",
        "\n",
        "total_samples = len(y_train_tensor)\n",
        "num_classes = len(class_counts)\n",
        "\n",
        "# Calculate weights using inverse frequency (more robust method)\n",
        "class_weights = {}\n",
        "for cls in range(num_classes):\n",
        "    count = class_counts.get(cls, 0)\n",
        "    if count > 0:\n",
        "        class_weights[cls] = total_samples / (num_classes * count)\n",
        "    else:\n",
        "        class_weights[cls] = 1.0  # fallback for missing classes\n",
        "\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# --- 2. Create Sample Weights Correctly ---\n",
        "# Create a weight for each sample in the training set\n",
        "sample_weights = torch.zeros(len(y_train_tensor), dtype=torch.float32)\n",
        "\n",
        "for idx, label in enumerate(y_train_tensor):\n",
        "    sample_weights[idx] = class_weights[label.item()]\n",
        "\n",
        "# Verify weights are valid\n",
        "print(f\"Sample weights - min: {sample_weights.min():.4f}, max: {sample_weights.max():.4f}, sum: {sample_weights.sum():.4f}\")\n",
        "print(f\"Any negative weights: {torch.any(sample_weights < 0)}\")\n",
        "print(f\"Any zero weights: {torch.any(sample_weights == 0)}\")\n",
        "\n",
        "# Normalize weights to prevent numerical issues\n",
        "if sample_weights.sum() > 0:\n",
        "    sample_weights = sample_weights / sample_weights.sum()\n",
        "else:\n",
        "    # Fallback: use uniform weights\n",
        "    sample_weights = torch.ones(len(y_train_tensor), dtype=torch.float32) / len(y_train_tensor)\n",
        "    print(\"笞ｸ  Using uniform weights as fallback\")\n",
        "\n",
        "print(f\"After normalization - min: {sample_weights.min():.4f}, max: {sample_weights.max():.4f}, sum: {sample_weights.sum():.4f}\")\n",
        "\n",
        "# --- 3. Create Weighted Random Sampler ---\n",
        "sampler = torch.utils.data.WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "print(\"笨 Weighted Random Sampler created successfully.\")\n",
        "\n",
        "# --- 4. Create DataLoaders with Fixed Sampler ---\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "NUM_WORKERS = 2  # Reduced for stability\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sampler=sampler,\n",
        "    shuffle=False,  # Important: shuffle must be False when using sampler\n",
        "    drop_last=True,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    pin_memory=True,\n",
        "    num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "print(\"笨 DataLoaders created successfully.\")\n",
        "print(f\"Train loader: {len(train_loader)} batches\")\n",
        "print(f\"Val loader: {len(val_loader)} batches\")\n",
        "print(f\"Test loader: {len(test_loader)} batches\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "I-WoASj-tU3L",
        "outputId": "3616819b-2523-4dbb-bd9f-e2dd96f59481"
      },
      "outputs": [],
      "source": [
        "# Ultra-simple training loop\n",
        "def simple_train():\n",
        "    model.train()\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, labels in train_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            _, outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Stats\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for data, labels in val_loader:\n",
        "                data, labels = data.to(device), labels.to(device)\n",
        "                _, outputs = model(data)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "# Run simple training\n",
        "print(\"Starting simplified training...\")\n",
        "trained_model = simple_train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV7_hyxPOgkJ",
        "outputId": "fe513022-9835-4302-c4db-da0e9031ef50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Head Training (10 epochs) ---\n",
            "HEAD EPOCH 1/10 | LR: 0.001000 | Train Acc: 46.87% | Val Acc: 48.02%\n",
            "HEAD EPOCH 2/10 | LR: 0.001000 | Train Acc: 48.47% | Val Acc: 47.92%\n",
            "HEAD EPOCH 3/10 | LR: 0.001000 | Train Acc: 48.36% | Val Acc: 48.23%\n",
            "HEAD EPOCH 4/10 | LR: 0.001000 | Train Acc: 48.27% | Val Acc: 48.34%\n",
            "HEAD EPOCH 5/10 | LR: 0.000500 | Train Acc: 48.31% | Val Acc: 48.66%\n",
            "HEAD EPOCH 6/10 | LR: 0.000500 | Train Acc: 48.42% | Val Acc: 48.37%\n",
            "HEAD EPOCH 7/10 | LR: 0.000500 | Train Acc: 48.27% | Val Acc: 48.37%\n",
            "HEAD EPOCH 8/10 | LR: 0.000500 | Train Acc: 48.51% | Val Acc: 48.50%\n",
            "HEAD EPOCH 9/10 | LR: 0.000500 | Train Acc: 48.36% | Val Acc: 48.05%\n",
            "HEAD EPOCH 10/10 | LR: 0.000250 | Train Acc: 48.44% | Val Acc: 48.60%\n",
            "**Head Training Finished. Best Validation Accuracy: 48.66%**\n"
          ]
        }
      ],
      "source": [
        "# --- 2. HEAD TRAINING HYPERPARAMETERS ---\n",
        "HEAD_EPOCHS = 10    # Short training phase\n",
        "HEAD_LR = 1e-3      # Higher Learning Rate\n",
        "STEP_SIZE_HEAD = 5\n",
        "GAMMA_HEAD = 0.5\n",
        "\n",
        "optimizer_head = optim.AdamW(model.parameters(), lr=HEAD_LR)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler_head = optim.lr_scheduler.StepLR(optimizer_head, step_size=STEP_SIZE_HEAD, gamma=GAMMA_HEAD)\n",
        "\n",
        "print(f\"\\n--- Starting Head Training ({HEAD_EPOCHS} epochs) ---\")\n",
        "\n",
        "best_val_accuracy_head = 0.0\n",
        "\n",
        "for epoch in range(1, HEAD_EPOCHS + 1):\n",
        "    # Training step (using the same 'train' function defined earlier)\n",
        "    train_loss, train_acc, current_lr = train(\n",
        "        model, train_loader, criterion, optimizer_head, device, scheduler_head\n",
        "    )\n",
        "\n",
        "    # Validation step (using the same 'validate' function defined earlier)\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(\n",
        "        f\"HEAD EPOCH {epoch}/{HEAD_EPOCHS} | LR: {current_lr:.6f} \"\n",
        "        f\"| Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "    if val_acc > best_val_accuracy_head:\n",
        "        best_val_accuracy_head = val_acc\n",
        "        torch.save(model.state_dict(), 'best_head_trained_model.pth')\n",
        "\n",
        "print(f\"**Head Training Finished. Best Validation Accuracy: {best_val_accuracy_head:.2f}%**\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuNkLYeJOmNY",
        "outputId": "2972f124-5663-40d6-fc63-989d7edbb9cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Differential Full Fine-Tuning (50 epochs) ---\n",
            "Backbone LR: 1.0e-05, Head LR: 5.0e-05\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# --- 1. Unfreeze All Layers ---\n",
        "# Ensure all parts of the model (Backbone and Head) are trainable\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# --- 2. Define Layer Groups and Differential LRs ---\n",
        "\n",
        "# Hyperparameters for Differential Fine-Tuning\n",
        "BACKBONE_LR = 1e-5    # Very Low LR for the pre-trained layers (e.g., 0.00001)\n",
        "HEAD_LR = 5e-5        # Higher LR for the new classification head (e.g., 0.00005)\n",
        "FULL_EPOCHS = 50      # Run for a sufficient number of epochs\n",
        "STEP_SIZE_FULL = 10\n",
        "GAMMA_FULL = 0.5\n",
        "\n",
        "# Collect parameters for the head and backbone separately\n",
        "params_to_update = [\n",
        "    # Group 1: The Classification Head (needs faster learning)\n",
        "    {'params': model.clshead.parameters(), 'lr': HEAD_LR},\n",
        "\n",
        "    # Group 2: The rest of the Model (Embedding, Transformer Encoder)\n",
        "    # Use the lower LR for fine-tuning the pre-trained weights\n",
        "    {'params': (p for name, p in model.named_parameters() if 'clshead' not in name), 'lr': BACKBONE_LR},\n",
        "]\n",
        "\n",
        "# --- 3. Setup Optimizer and Scheduler ---\n",
        "optimizer_full = optim.AdamW(params_to_update)\n",
        "criterion = nn.CrossEntropyLoss() # Assuming this was already defined\n",
        "scheduler_full = optim.lr_scheduler.StepLR(optimizer_full, step_size=STEP_SIZE_FULL, gamma=GAMMA_FULL)\n",
        "\n",
        "print(f\"\\n--- Starting Differential Full Fine-Tuning ({FULL_EPOCHS} epochs) ---\")\n",
        "print(f\"Backbone LR: {BACKBONE_LR:.1e}, Head LR: {HEAD_LR:.1e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "qxI3_63JT-Dh",
        "outputId": "5e9318be-694b-4c6a-a55b-266bf6acbbed"
      },
      "outputs": [],
      "source": [
        "# --- 1. Define Optimizer Groups ---\n",
        "BACKBONE_LR_NEW = 5e-5    # Aggressive LR for pre-trained layers\n",
        "HEAD_LR_NEW = 1e-4        # Higher LR for the new classification head\n",
        "FULL_EPOCHS = 70          # Extended training time\n",
        "BEST_MODEL_PATH = 'best_aggressive_ft_model.pth'\n",
        "\n",
        "# Ensure the model is moved to device (assumes 'model' and 'device' are defined)\n",
        "model.to(device)\n",
        "\n",
        "# Ensure all layers are unfreezed\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "params_to_update_aggressive = [\n",
        "    # Group 1: Classification Head (Highest LR)\n",
        "    {'params': model.clshead.parameters(), 'lr': HEAD_LR_NEW},\n",
        "\n",
        "    # Group 2: Backbone (Higher Adaptation LR)\n",
        "    {'params': (p for name, p in model.named_parameters() if 'clshead' not in name), 'lr': BACKBONE_LR_NEW},\n",
        "]\n",
        "\n",
        "# --- 2. Setup Optimizer and Scheduler ---\n",
        "optimizer_aggressive = optim.AdamW(params_to_update_aggressive)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler_aggressive = optim.lr_scheduler.StepLR(optimizer_aggressive, step_size=10, gamma=0.5)\n",
        "\n",
        "print(f\"\\n--- Starting AGGRESSIVE Differential Fine-Tuning ({FULL_EPOCHS} epochs) ---\")\n",
        "print(f\"NEW Backbone LR: {BACKBONE_LR_NEW:.1e}, NEW Head LR: {HEAD_LR_NEW:.1e}\")\n",
        "\n",
        "\n",
        "# --- 3. Training Loop ---\n",
        "best_val_accuracy_full = 0.0\n",
        "\n",
        "for epoch in range(1, FULL_EPOCHS + 1):\n",
        "    # Call the user-defined train function\n",
        "    train_loss, train_acc, current_head_lr = train(\n",
        "        model, train_loader, criterion, optimizer_aggressive, device, scheduler_aggressive\n",
        "    )\n",
        "\n",
        "    # Call the user-defined validate function\n",
        "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
        "\n",
        "    print(\n",
        "        f\"FULL EPOCH {epoch}/{FULL_EPOCHS} | Head LR: {current_head_lr:.7f} \"\n",
        "        f\"| Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\"\n",
        "        f\"| Train loss: {train_loss:.4f}% | Val loss: {val_loss:.4f}%\"\n",
        "    )\n",
        "\n",
        "    if val_acc > best_val_accuracy_full:\n",
        "        best_val_accuracy_full = val_acc\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\"**Saved new best model with Val Acc: {best_val_accuracy_full:.2f}%**\")\n",
        "\n",
        "print(f\"\\nFull Fine-Tuning finished! Final Best Validation Accuracy: {best_val_accuracy_full:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17WxzmcLO5Ym"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# --- 5. Final Evaluation on Test Set ---\n",
        "\n",
        "# Load the best fine-tuned model weights\n",
        "try:\n",
        "    model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
        "    print(\"\\n笨 Loaded best fine-tuned model for final evaluation.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"\\n笞ｸ Warning: Best fine-tuned model file not found. Evaluating with current model state.\")\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "# Evaluation loop\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        _, outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "final_test_loss = test_loss / len(test_loader)\n",
        "final_test_accuracy = test_correct / test_total * 100\n",
        "conf_matrix = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "print(\"\\n--- Final Test Results ---\")\n",
        "print(f\"Test Loss: {final_test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: **{final_test_accuracy:.2f}%**\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
